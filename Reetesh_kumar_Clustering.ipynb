import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score
from sklearn.decomposition import PCA

customers_df = pd.read_csv('Customers.csv')
products_df = pd.read_csv('Products.csv')
transactions_df = pd.read_csv('Transactions.csv')

feature_matrix = pd.read_csv('feature_matrix.csv')
features_for_clustering = feature_matrix.drop('CustomerID', axis=1)

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_for_clustering)

db_scores = []
k_values = range(2, 11)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    clusters = kmeans.fit_predict(features_scaled)
    db_score = davies_bouldin_score(features_scaled, clusters)
    db_scores.append(db_score)

plt.figure(figsize=(10, 6))
plt.plot(k_values, db_scores, 'bo-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Davies-Bouldin Index')
plt.title('Davies-Bouldin Score vs. Number of Clusters')
plt.show()

optimal_k = k_values[np.argmin(db_scores)]
final_kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = final_kmeans.fit_predict(features_scaled)

clustered_data = feature_matrix.copy()
clustered_data['Cluster'] = clusters

cluster_analysis = clustered_data.groupby('Cluster').agg({
    'CustomerID': 'count',
    'TotalSpend': 'mean',
    'TransactionCount': 'mean'
}).round(2)

pca = PCA(n_components=2)
features_2d = pca.fit_transform(features_scaled)

plt.figure(figsize=(12, 8))
scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], c=clusters, cmap='viridis')
plt.title('Customer Segments Visualization (PCA)')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.colorbar(scatter, label='Cluster')
plt.show()
